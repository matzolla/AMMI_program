
- One advantage of the RELU activation function max(0,z) with z=w^T@x+b is that we can always perform gd since whenever the unit is active a disadvantage we can't learnwith a zero derivative (no updates) to overcome this we introduced leaky relu for example.

- Th
