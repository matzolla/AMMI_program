
## Foundation of DeepLearning
- One advantage of the RELU activation function max(0,z) with z=$w^{T}$x+b is that we can always perform gd since whenever the unit is active a disadvantage we can't learnwith a zero derivative (no updates) to overcome this we introduced leaky relu for example.

- The sigmoid and tanh functions looks alike `tanh(x)=2sigma(2x)-1` and `$sigma(x)=\frac{1}{1+exp(-x)}$`
